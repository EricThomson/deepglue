{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c234dd0e-abf7-4b35-b565-aaa8a28fc346",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a8f7e-1db0-4c46-83a5-e1a3f2c331f9",
   "metadata": {},
   "source": [
    "# 0. Intro: deepglue fashion-mnist demo\n",
    "<img src=\"https://raw.githubusercontent.com/EricThomson/deepglue/main/docs/images/deep_glue_logo.png\" alt=\"deepglue logo\" align=\"right\" width=\"160\" style=\"margin-left:10px;\">\n",
    "\n",
    "Welcome to the deepglue fashion-mnist demo! If you are new to deepglue, this is the best place to start, as it introduces the key features and functions.\n",
    "\n",
    "deepglue is a library of pytorch utilities designed to simplify and streamline deep learning workflows (our motto is \"Keeping the  useful stuff together in one place\"). In this demo, we'll highlight some of deepglue's key features using the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset, which contains 70,000 images of clothing items organized into 10 categories (e.g., t-shirts). Because this demo is meant to run in multiple environments, including Google Colab, we will be using a small subset of fashion mnist with only 3,000 images (`fashion3k`).\n",
    "  \n",
    "The main steps we'll walk through in this notebook include:\n",
    "- **Set up project**: Create directory structure for project.\n",
    "- **Download and explore dataset**: Retrieve and explore the fashion3k dataset.\n",
    "- **Define the network**: Set up resnet18 convolutional network for transfer learning.\n",
    "- **Set up data for training**: Define augmentation transforms and data loaders for training. \n",
    "- **Train and evaluate the model**: Train the model on fashion3k, and evaluate performance on validation data.\n",
    "- **Visualize Features**: Visualize feature clusters to help us understand the network's behavior.\n",
    "  \n",
    "If anything is unclear in this demo, feel free to ask questions at the repo's [Discussion forum](https://github.com/EricThomson/deepglue/discussions). If you find a problem, please [raise an issue](https://github.com/EricThomson/deepglue/issues). \n",
    "\n",
    "Also, in the rest of the notebook you can explore any function's documentation by typing the function name followed by a question mark. E.g.,  just type in `dg.train_one_epoch?` in a code cell to get a printout of the documentation for `train_one_epoch()`. To dig deeper, you can also explore [deepglue's source code](https://github.com/EricThomson/deepglue) and [online documentation](https://deepglue.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a839c",
   "metadata": {},
   "source": [
    "### Import packages\n",
    "Let's get started by importing the packages we'll use in the rest of the notebook.\n",
    "\n",
    "For those in Colab, we first need to install packages that are not natively part of the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c56ad-82b1-4ed0-994d-c64619834c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab # noqa: F401 # ruff: ignore unused import error \n",
    "    in_colab = True\n",
    "except ImportError:\n",
    "    in_colab = False\n",
    "\n",
    "if in_colab:\n",
    "    print(\"Installing deepglue and umap-learn for colab envirnoment\")\n",
    "    !pip install -q deepglue umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7df6e3-5315-406d-a2c6-f5be8db1aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "from IPython.display import HTML\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "import umap\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "import deepglue as dg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e3989-e9c7-4250-a36d-0b02480a637b",
   "metadata": {},
   "source": [
    "Determine whether to use CPU or GPU to train. If your pytorch install detects a GPU, then set your device to `cuda`, otherwise default to `cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f4042-ea44-4699-8e87-8f897cf8c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645db40f-18bb-4c81-aabf-159f341473d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71227e4-75ef-4ba2-9afe-5dbabb7f6670",
   "metadata": {},
   "source": [
    "Deepglue has loggers set up through many of its functions that print out what it is doing internally. For now, we will set the logger to just print warnings. To get more informative outputs you can set it to `INFO`.\n",
    "\n",
    "If you don't want your world cluttered with logging messages, you can delete the following or comment it out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79384ef2-0e40-493d-b00a-74349f5cd133",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_format = \"%(asctime)s - %(filename)s - %(funcName)s - line %(lineno)d - %(levelname)s - %(message)s\"\n",
    "logging.basicConfig(level=logging.WARNING,  # INFO DEBUG WARNING ERROR etc\n",
    "                    format=log_format,\n",
    "                    force=True)  # to override colab defaults "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48b7d55-9768-495d-b1ef-1449c4783bba",
   "metadata": {},
   "source": [
    "# 1. Set up project structure \n",
    "It's key to be organized for any deep learning project. Here, we will set up a directory structure for our fashion mnist project. \n",
    "\n",
    "## Set up directory for all projects\n",
    "Since this is the first demo, we'll set up the default `projects` directory where all deepglue projects will be stored. \n",
    "\n",
    "The following will create a `.deepglue/projects/` directory in your system's home directory. The exact location will depend on your OS and environment (e.g., on Linux/Mac it will be `~`, on Windows it will be `C:/Users/Username/`, and in Colab it will be `root/`). If you prefer a different base directory, you can change `deepglue_dir` to a different value below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548ed78-6995-4140-8046-639f2b0e0e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepglue_dir = Path.home() / \".deepglue\"\n",
    "projects_dir = deepglue_dir / \"projects\"\n",
    "\n",
    "try:\n",
    "    projects_dir.mkdir(parents=True, exist_ok=False) # prevent overwriting\n",
    "except FileExistsError:\n",
    "    print(\"'projects/' directory already exists. Skipping creation.\\n\")\n",
    "\n",
    "print(f\"Your deepglue projects directory is: {projects_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96347a9b-1f62-4f64-8cfb-bb8e13c71a7a",
   "metadata": {},
   "source": [
    "## Directory for fashion project\n",
    "Within the deepglue project folder, let's create the minimal directory structure for our fashion mnist classification project. Given a new project name (`fashion`), deepglue's `create_project()` function creates the following directory structure within `projects/`:\n",
    "\n",
    "    fashion/\n",
    "        data/\n",
    "        models/\n",
    "\n",
    "This is a kind of minimal structure for a deep learning project. You can add more as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccb239f-47d3-4828-a118-5dc72239c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"fashion\"\n",
    "project_dir, project_data_dir, project_models_dir = dg.create_project(projects_dir, project_name)\n",
    "\n",
    "print(f\"Your new project directory: {project_dir}\")\n",
    "print(f\"Datasets will go into {project_data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6fae0-e08b-4d21-a250-eb977c6e6f94",
   "metadata": {},
   "source": [
    "# 2. Download data\n",
    "As mentioned above, we have a subsampled version of the full fashion mnist dataset. It has been split into training, validation, and test data that is stored online:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaafa11-f9ea-472c-9c54-e6f8d2c6a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = r'https://drive.google.com/uc?id=1tBiL2H9xcjy7ClkLsxuag4_gBKusau1J'\n",
    "filename = 'fashion3k.zip'\n",
    "data_zip_path = project_data_dir / filename\n",
    "data_dir = project_data_dir / 'fashion3k' # data dir we'll unzip into\n",
    "print(f\"Will attempt download to {data_zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a6f1a-e45d-41a2-9b54-04ceb327f5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not data_zip_path.exists():\n",
    "    print(f\"Downloading {filename}. This can take a minute.\")\n",
    "    gdown.download(data_url, str(data_zip_path), quiet=False, fuzzy=True, use_cookies=True)\n",
    "else:\n",
    "    print(f\"{filename} already downloaded. Download skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a45050c-22c2-49e1-a92b-bf16f3df9efa",
   "metadata": {},
   "source": [
    "Unzip the compressed data into the `data_dir` we already created above. The following has some extra wrinkles to avoid errors and repeating decompression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db6be9f-39d3-4260-af1a-adf3d14df5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if data_dir.exists():\n",
    "    print(\"Already extracted. Skipping.\")\n",
    "else:\n",
    "    if data_zip_path.suffix == '.zip':\n",
    "        print(f\"Unzipping to {project_data_dir}...\")\n",
    "        with zipfile.ZipFile(data_zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(project_data_dir)\n",
    "        print(\"Done unzipping!\\n\")\n",
    "    else:\n",
    "        print('Not a zip file.')\n",
    "\n",
    "train_dir = data_dir / 'train'\n",
    "valid_dir = data_dir / 'valid'\n",
    "test_dir = data_dir / 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d2e97b-1203-4b53-86f9-ca5235f04705",
   "metadata": {},
   "source": [
    "Our final directory structure follows a standard that is used by several `pytorch` functions:\n",
    "\n",
    "        projects/\n",
    "            fashion/\n",
    "                models/\n",
    "                data/\n",
    "                    fashion3k/\n",
    "                        train/\n",
    "                            0/  [tshirt]\n",
    "                            1/  [trouser]\n",
    "                        valid/\n",
    "                            0/\n",
    "                            1/   \n",
    "                        test/\n",
    "                            0/\n",
    "                            1/\n",
    "\n",
    "Each end node in this directory tree (`0/`) contains image data from the relevant category: feel free to navigate to the relevant spots on your machine to check out the data. Below we will show how to use `deepglue` to inspect random samples of the data. \n",
    "\n",
    "Our mapping from subdirectory names to actual categories is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef936f98-2143-448a-955b-f14993a412c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map = {'0': 'tshirt',\n",
    "                '1': 'trouser',\n",
    "                '2': 'pullover',\n",
    "                '3': 'dress',\n",
    "                '4': 'coat',\n",
    "                '5': 'sandal',\n",
    "                '6': 'shirt',\n",
    "                '7': 'sneaker',\n",
    "                '8': 'bag',\n",
    "                '9': 'ankle_boot'}\n",
    "\n",
    "categories = ['0','1','2','3','4','5','6','7','8','9']\n",
    "category_names = [category_map[key] for key in categories]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c621603-dc1f-442e-8892-66603d3f92a4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3 style=\"margin: 1px 0 6px 0;\">Scalable data structures</h3>\n",
    "In real-world projects there will often be multiple datasets placed in the project's <code>data/</code> directory. Since we are working with a single dataset, we could in theory just extract the <code>train/</code>, <code>valid/</code> and <code>test/</code> folders directly into <code>data/</code> to keep a more flat structure. But our more nested structure mimics how larger projects handle multiple datasets in a scalable way, so we'll stick with it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f62e8b-4e9d-438e-9c7c-3a31bee3a4e2",
   "metadata": {},
   "source": [
    "# 3. Explore data \n",
    "An initial look at the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a87d1-bfdd-4f05-b16e-ce22798f4136",
   "metadata": {},
   "source": [
    "Deepglue will plot some random images from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bd38a2-a242-4b2f-84b6-033fb606fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg.plot_random_sample(data_dir, category_map, split_type='train', num_to_plot=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d15f51e-0150-4bb1-9472-dd4bcb63f29e",
   "metadata": {},
   "source": [
    "Within the project, the data are divided into training, validation, and testing splits. How many are in each split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a2242-58db-4c2b-8de7-d4d323d413b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_per_split = dg.count_by_split(data_dir)\n",
    "\n",
    "total_samples = num_per_split['train'] + num_per_split['valid'] + num_per_split['test']\n",
    "print(f\"Num samples total: {total_samples}\")\n",
    "pprint(num_per_split)\n",
    "data_splits = ['train', 'valid', 'test']\n",
    "proportion_per_split =  [num_per_split['train']/total_samples, \n",
    "                         num_per_split['valid']/total_samples, \n",
    "                         num_per_split['test']/total_samples]\n",
    "\n",
    "# Plot number in train/validation/test splits\n",
    "plt.bar(data_splits, proportion_per_split)\n",
    "plt.title(\"Proportion per split\")\n",
    "plt.xlabel('Split Type')\n",
    "plt.ylabel('Proportion');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79078ca-2ffb-401b-a491-3cb778127cd6",
   "metadata": {},
   "source": [
    "In this tiny sample of `fashion_mnist`, we have 3000 total samples, with 2100 images set aside for training, 450 for validation and testing respectively (a 70/15/15 split). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5375dd-80d0-440a-852a-e46226dfa646",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3 style=\"margin: 1px 0 6px 0;\">fashion3k versus fashion_mnist</h3>\n",
    "\n",
    "<p>Because fashion3k is a much smaller dataset than the full fashion-mnist (70,000 images), this lets us train our models even in runtime environments without a great deal of computational resources (e.g., free-tier Colab). The tradeoff is that we will be sacrificing accuracy. We made this tradeoff because the main point of this demo is to quickly illustrate key concepts, not to maximize accuracy.</p>\n",
    "\n",
    "<p>If you want more data, we discuss how to download the full fashion mnist dataset at the end of this notebook.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f427ce0-eaef-4dd4-8c58-49165d804c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_category_by_split = dg.count_category_by_split(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90529816-7472-4bd3-84dc-a3a95d8466fc",
   "metadata": {},
   "source": [
    "`count_category_by_split()` creates a dict with the splits as the keys (`train`, `valid`, and `test`), and each dictionary contains the number of samples from each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8f574-e23d-4a43-9955-d55d04ed26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training split counts:')\n",
    "pprint(num_category_by_split['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03000b90-6146-41ee-9cfb-5e9f9cf07fcc",
   "metadata": {},
   "source": [
    "Let's plot the proportion represented in the categories in the three data splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9851e7a-0256-4a92-8911-7fcc975a44e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counts and proportions\n",
    "train_counts = np.array([num_category_by_split['train'][key] for key in categories])\n",
    "train_proportions = train_counts/sum(train_counts)\n",
    "\n",
    "valid_counts = np.array([num_category_by_split['valid'][key] for key in categories])\n",
    "valid_proportions = valid_counts/sum(valid_counts)\n",
    "\n",
    "test_counts = np.array([num_category_by_split['test'][key] for key in categories])\n",
    "test_proportions = test_counts/sum(test_counts)\n",
    "\n",
    "# plot them\n",
    "fig, (ax_test, ax_val, ax_train) = plt.subplots(3,1,figsize=(5,5)) # width x height\n",
    "\n",
    "#train (bottom)\n",
    "ax_train.bar(category_names, train_proportions)\n",
    "ax_train.tick_params(axis='x', labelrotation=45)\n",
    "ax_train.set_title(\"Training Data\")\n",
    "ax_train.set_xlabel('Category')\n",
    "\n",
    "# validation (middle)\n",
    "ax_val.bar(category_names, valid_proportions)\n",
    "ax_val.set_ylabel('Proportion');\n",
    "ax_val.set_title(\"Validation Data\")\n",
    "ax_val.set_xticks([]) \n",
    "\n",
    "# test (top)\n",
    "ax_test.bar(category_names, test_proportions)\n",
    "ax_test.set_title(\"Test Data\")\n",
    "ax_test.set_xticks([])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cdea37-d23c-45df-9c39-0bf8d4291447",
   "metadata": {},
   "source": [
    "This is (by design) an extremely balanced data set, in all three splits. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24308eac-4643-4251-8790-66b93493a40e",
   "metadata": {},
   "source": [
    "# 4. Define network model\n",
    "To learn the fashion classificaiton task, we'll start with a pre-trained [resnet18 model](https://www.run.ai/guides/deep-learning-for-computer-vision/pytorch-resnet) that was trained on the Imagenet 1k dataset (1000 categories with over 1 million total images). We are initially freezing all the model parameters, and then will unfreeze the final two convolution layers, and add in a new fully connected (fc) layer that includes 50% dropout to prevent overfitting (dropout is only turned on during training). \n",
    "\n",
    "We'll be keeping the early layers which extract basic features, but allowing the later layers to learn the higher-level fashion-specific features in fashion-mnist. \n",
    "\n",
    "Feel free to substitute a different model and adapt the code to suit your needs. For instance, `resnet50` performs better, but it will also take up way more memory and take longer to train. For this little demo, the smaller and faster `resnet18` performs good enough. We'll discuss the topic of trying out different models at the end in the `Test data` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d1c3c6-2fa0-4eb6-830e-ab530921ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(category_map)  # Adjust to match the number of classes in Fashion MNIST\n",
    "num_fc_hidden_units = 128  # Number of hidden units in fully connected layer: feel free to tweak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e17b94a-83e0-4590-a1ff-a1997c21892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet50 model\n",
    "resnet_weights = models.ResNet18_Weights.IMAGENET1K_V1\n",
    "resnet18 = models.resnet18(weights=resnet_weights)\n",
    "\n",
    "# Freeze all model parameters initially\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# unfreeze blocks 3 and 4\n",
    "for param in resnet18.layer3.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "for param in resnet18.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Get the number of inputs for the original fully connected (fc) layer\n",
    "num_fc_inputs = resnet18.fc.in_features\n",
    "\n",
    "# Replace the final fully connected layer (note requires_grad is True by default for a brand-new layer)\n",
    "resnet18.fc = nn.Sequential(nn.Linear(num_fc_inputs, num_fc_hidden_units),  # Projection from backbone to hidden units\n",
    "                            nn.ReLU(),                                      # Activation for non-linearity\n",
    "                            nn.Dropout(p=0.5),                             # Reduce overfitting\n",
    "                            nn.Linear(num_fc_hidden_units, num_classes))    # Final layer for class prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6441f7c5-fb9b-428c-ab4a-2d905691253c",
   "metadata": {},
   "source": [
    "We also need to define a loss function and optimizer (which includes the learning rate schedule) for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a9ba93-013b-467a-857b-6bcd149b9d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(resnet18.parameters(), lr=0.0001)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2e8f6e-6c4f-4e7d-9b02-520881044d56",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3 style=\"margin: 1px 0 6px 0;\">Expected inputs for resnet</h3>\n",
    "Resnet models were trained on 224 x 224 images, and this matters. If you try to feed the model different sized images, it will not perform as well, or you may get errors at some point in your project. Fashion mnist images are 28x28, so in what follows we will upsample them to 224x224. We'll also convert the inputs to RGB. We could define a completely new network and train from scratch, but then we'd lose out on the gains from the pretrained network. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5170480-66c2-4db2-918c-1062e857cdf3",
   "metadata": {},
   "source": [
    "# 5. Set up data for training\n",
    "Having a bunch of data in folders is great, but `torchvision` provides lots of utilities to make funneling such data through training pipelines really easy. Also, integrating transformations like random cropping, rotations, and normalization directly into the our pipelines simplifies training, and `torchvision` has a great api that integrates such augmentations directly into the data pipeline. \n",
    "\n",
    "Defining datasets and data loaders requires us first to define the transforms we will use on the data as we load it from disk. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93818e02-925c-40ce-8ac1-66f8fd2fc6c4",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "Torchvision has an amazing set of transforms that apply whether you are doing classification, object detection, or scene segmentation. To learn more about the transforms, see their [documentation](https://pytorch.org/vision/main/transforms.html#v2-api-ref) or [example page](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py). \n",
    "\n",
    "We are going to set up a relatively simple transform here just to show the logic. First we'll define a couple of transforms that we will randomly apply in our transform function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a43bfd0-9111-43d3-9206-7fd79143a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_noise = transforms.GaussianNoise(mean=0, # mean of sampled noise\n",
    "                                         sigma=0.1, # std of sampled noise\n",
    "                                         clip=True)  # clip to [0,1] after adding noise\n",
    "gaussian_blur = transforms.GaussianBlur(kernel_size=(7,7), # kernel size (width, height)\n",
    "                                        sigma=(0.8, 0.8)) # sigma: min, max randomly chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f94c68-88ac-4133-acb4-b33ae0b561e1",
   "metadata": {},
   "source": [
    "Then define a function for a data transform that can be applied to different data splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a82c3c-b5bd-45da-8be1-fd956e257418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transform(train=False):\n",
    "    \"\"\"\n",
    "    Define data transformation to be used when data is ingested for use by datasets.\n",
    "    \"\"\"\n",
    "    transform_pipeline = []\n",
    "    \n",
    "    transform_pipeline.append(transforms.Resize((224, 224)))     # resnet expect this size\n",
    "    transform_pipeline.append(transforms.ToImage())  # Convert PIL image to tensor: many transforms only work on torch tensor\n",
    "    transform_pipeline.append(transforms.ToDtype(torch.float32, scale=True)) # many transforms only work with float\n",
    "    transform_pipeline.append(transforms.RGB()) # resnet expects RGB (if it is already RGB, nothing changes)\n",
    "\n",
    "    if train:\n",
    "        transform_pipeline.append(transforms.RandomApply([gaussian_noise], p=1/3)) # will apply p of the time\n",
    "        transform_pipeline.append(transforms.RandomApply([gaussian_blur], p=1/3))  \n",
    "        transform_pipeline.append(transforms.RandomHorizontalFlip(p=1/2))\n",
    "        transform_pipeline.append(transforms.RandomRotation(30, fill=0.445, expand=False))  # will rotate (-20,20); expand would resize so image fits in image shape\n",
    "        \n",
    "    # Normalize to standard values for resnet\n",
    "    transform_pipeline.append(transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                   std=[0.229, 0.224, 0.225]))  \n",
    "    return transforms.Compose(transform_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f1ff68-5efe-422a-a9c8-5651c5b52170",
   "metadata": {},
   "source": [
    "In the above, we have defined a general transform function that we can define for diffrent data splits: those that are set for training data will have distortions applied to the data for augmentation purposes (gaussian blur, noise, flips, rotations, etc.). The validation and test splits will only have \"nondistorting\" changes applied: they will be rescaled to the proper size, changed to torch tensors, converted to floats, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4523831-ddc0-4a3c-ab5b-bf4686b06b44",
   "metadata": {},
   "source": [
    "### Transform demo\n",
    "Let's look at how the transform works in the case of training data just for fun. We'll set train to `True` so we'll see the effects of the augmenting transforms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144d3a8-91bf-4488-837b-a83d172e7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transform = data_transform(train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7656affe-80a3-4536-a104-b2d75d683e23",
   "metadata": {},
   "source": [
    "Let's get a random image from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d9b06-e3d7-4d06-afb2-3ec00a11127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_plot = '7'\n",
    "demo_image_path, demo_image_category = dg.sample_random_images(data_dir, \n",
    "                                                               category_map, \n",
    "                                                               category=category_to_plot) \n",
    "demo_image = np.array(Image.open(demo_image_path[0]))\n",
    "dg.plot_transformed(demo_image, demo_transform, num_to_plot=9);\n",
    "print(category_map[category_to_plot])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7669f1d7-61fe-41f9-b378-ffd1fef552ef",
   "metadata": {},
   "source": [
    "You can see that we get multiple views of the sneaker, some are pretty distorted. This is good! We want to give the network some tricky instances for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13af67fc-e89d-4c61-86d2-4758dd4aa028",
   "metadata": {},
   "source": [
    "### Define transforms for our datasets\n",
    "We need transforms for training, validation, and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc86b3fd-29c3-4a88-ae6b-231f786156ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = {'train': data_transform(train=True),\n",
    "                    'valid': data_transform(train=False),\n",
    "                    'test': data_transform(train=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6a89e-be6c-4337-83e8-b6ac17730800",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "Let's create our training, validation, and test datasets using torchvision's `datasets.ImageFolder` class. This is a convenience class that hooks into image folders that are structured the way we have set them up. In general, pytoch datasets provide a flexible way to define how your data is loaded, transformed, and accessed, making it useful for deep learning workflows. \n",
    "\n",
    "Datasets are designed to load data lazily, meaning they load individual items on-the-fly when accessed: this minimizes memory usage, which is especially helpful fo large datasets. The primary use of datasets is that they get wrapped into a `DataLoader`, which we will see in the next section. \n",
    "\n",
    "We'll create a dictionary of datasets, one dataset for each data split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d465027-6a12-4995-b3c3-1d3539d82419",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'train': datasets.ImageFolder(root=train_dir, transform=image_transforms['train']),\n",
    "        'valid': datasets.ImageFolder(root=valid_dir, transform=image_transforms['valid']),\n",
    "        'test': datasets.ImageFolder(root=test_dir, transform=image_transforms['test'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8974bc-33a6-4b4a-bc7e-9e49d3c36a92",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "The point of the above transforms/datasets is to create data loaders, which is what we actually use directly during training. They shuffle the data, create batches, and parallelize the workload using multiprocessing to make everything go faster.\n",
    "\n",
    "Dataloaders require a couple of parameters and you might have to adjust these depending on your system. Just to review some ML terminology: one *epoch* is a single run through the entire data set. Our training data has 50k images, so one epoch of the training data will be a run through 50k images. This is way too much to run through all at once. A *batch* is the number of images in a subset for one round of processing during training: the error is calculated for this subset and the network is updated during training for each batch. `num_workers` is the number of (CPU) processes that will work to generate batches in parallel: such parallelization can *significantly* speed up runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b3020-a31f-42be-9de6-cab290aaf8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256   # 128 on laptop, 256 on workstation\n",
    "\n",
    "if in_colab:\n",
    "    num_workers = 2  # max CPUs available on free colab is 2\n",
    "else:\n",
    "    num_workers = 6   # 4 on laptop, 6 on workstation\n",
    "\n",
    "# persistent workers makes things startup faster, but only if you have multiple workers\n",
    "if num_workers > 0:\n",
    "    persist_workers = True\n",
    "else: \n",
    "    persist_workers = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a401465-0a31-4f3a-aee7-d7b87ec8cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(data['train'], \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=True,\n",
    "                               num_workers=num_workers,\n",
    "                               drop_last=True,  # drop dangling batch at end\n",
    "                               persistent_workers=persist_workers)\n",
    "\n",
    "valid_data_loader = DataLoader(data['valid'], \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=True,\n",
    "                               num_workers=num_workers,\n",
    "                               persistent_workers=persist_workers)\n",
    "\n",
    "test_data_loader = DataLoader(data['test'], \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True,\n",
    "                              num_workers=num_workers,\n",
    "                              persistent_workers=persist_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6432293-4e07-4a47-8401-7ea9073934f7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3 style=\"margin: 1px 0 6px 0;\">Multiprocessing on different operating systems</h3>\n",
    "Different systems can take extra time to create multiple workers. Windows and (newer) Macs uses a different method than Linux to create new workers. On some older Windows systems, setting <code>num_workers</code> to any number greater than <code>0</code> will simply cause your system to hang. If this happens, you unfortunately have to set <code>num_workers = 0</code>. However, do give it a minute to start, because once the workers are set up initially, the speed payoff is <em>significant</em>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74610d36-eca7-4019-b541-5f0005b04c98",
   "metadata": {},
   "source": [
    "If you have already trained a model, you can jump to step 7 -- load the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef90106-aa78-42ad-a224-bc82a3b917bf",
   "metadata": {},
   "source": [
    "# 6. Train the network\n",
    "We'll use deepglue's `train_and_validate()` to update the weights in the network using the training data, and check performance on the validation data. This is what we've been building toward! \n",
    "\n",
    "There are a couple of parameters we need to set. How many times will we cycle throught the datasets during training (`num_epochs`), and the `topk` accuracy values.\n",
    "\n",
    "A top-k prediction counts as correct if the prediction was in the top k highest probabilities from the network, even if it wasn't the top choice (e.g., if a network's highest estimates for an image is `[sneaker, sandal, ankle_boot]`, for a `sandal` then it is top-3 accurate). This is useful for datasets with lots of similar categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4420a2-30ae-4618-ab64-4c1eee96c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = (1,3)\n",
    "\n",
    "# very slow if only running on cpu\n",
    "if device == 'cpu':\n",
    "    num_epochs = 5\n",
    "else:\n",
    "    num_epochs = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa2ca9a-5b4c-4b78-852b-30dc1a644d15",
   "metadata": {},
   "source": [
    "This is the is the slowest step in the notebook. While we've tweaked the dataset to make it faster, it can still be slow on CPU-only systems so we set `num_epochs` to 5 on such systems. Feel free to increase it if you don't mind waiting.  Under the hood, `dg.train_and_validate()` works by churning through the data from the training and validation dataloaders, each `num_epoch` times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0acc6c7-0a99-47e8-907d-49678927a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  reset our logging level so we can get some feedback during training\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "trained_model, train_history = dg.train_and_validate(resnet18,\n",
    "                                                     train_data_loader,\n",
    "                                                     valid_data_loader,\n",
    "                                                     loss_func,\n",
    "                                                     optimizer,\n",
    "                                                     device=device,\n",
    "                                                     topk=topk,\n",
    "                                                     epochs=num_epochs);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff6f2d4-d0c2-4b51-81ec-12f22f191d2d",
   "metadata": {},
   "source": [
    "## View loss/accuracy\n",
    "Let's see how it did during training. Note we will explore other metrics below this is just to get a quick sense for how things went. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b5a18-7042-4f25-aed7-5c6dc8efaf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_array = np.arange(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a0535-fd0b-478a-8e6a-16d20db6ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(10,4))\n",
    "\n",
    "# Plot loss \n",
    "ax_loss.plot(epoch_array, train_history['train_loss'], color='blue',  label=\"Training Data\", marker='.')\n",
    "ax_loss.plot(epoch_array, train_history['val_loss'], color='firebrick', label=\"Validation Data\", marker='.');\n",
    "ax_loss.set_title('Training Loss')\n",
    "ax_loss.set_ylabel('Loss')\n",
    "ax_loss.set_xlabel('Epoch')\n",
    "ax_loss.legend();\n",
    "\n",
    "# Plot topk accuracies\n",
    "ax_acc.plot(epoch_array, train_history['val_topk_accuracy'][:,1], color='firebrick',  label=\"Validation top 3\", marker='.');\n",
    "ax_acc.plot(epoch_array, train_history['train_topk_accuracy'][:,1], color='blue', label=\"Training top 3\", marker='.')\n",
    "ax_acc.plot(epoch_array, train_history['val_topk_accuracy'][:,0],  color='darksalmon', label=\"Validation top 1\", marker='.');\n",
    "ax_acc.plot(epoch_array, train_history['train_topk_accuracy'][:,0],  color='lightsteelblue', label=\"Training top 1\", marker='.')\n",
    "\n",
    "ax_acc.axhline(y=100, color='k', linestyle='--', linewidth=0.5)\n",
    "ax_acc.set_title('Training Accuracy')\n",
    "ax_acc.set_ylabel('Accuracy')\n",
    "ax_acc.set_xlabel('Epoch')\n",
    "ax_acc.legend();\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9277fa91-10da-4a44-9578-ded4dc54e55f",
   "metadata": {},
   "source": [
    "Things look reasonable on a first pass. We are inspecting to see if validation data trends in the opposite direction of the training data, which would be a classic sign of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf6de90-01d1-4590-9bb3-e13c2c5f69db",
   "metadata": {},
   "source": [
    "# 7. Save the network\n",
    "Docs on this: https://pytorch.org/tutorials/beginner/saving_loading_models.html \n",
    "\n",
    "Temporary until we have proper checkpoint save/load built into training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bbe46b-4538-4918-b37a-3040eb255a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_save_name = r\"resnet18_final.pth\" #\n",
    "checkpoint_save_path = project_models_dir / checkpoint_save_name\n",
    "checkpoint_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac193ce-32f4-45ca-8de8-10b7e3af8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model': trained_model,   # Model parameters\n",
    "            'optimizer': optimizer,  # Optimizer parameters\n",
    "            'epochs': num_epochs,  \n",
    "            'train_loss': train_history['train_loss'],\n",
    "            'val_loss': train_history['val_loss'],  \n",
    "            'train_accuracy': train_history['train_topk_accuracy'],\n",
    "            'val_accuracy': train_history['val_topk_accuracy'],\n",
    "            'topk': topk,}, checkpoint_save_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1785e9a-1045-4568-8ec9-3a336712a3db",
   "metadata": {},
   "source": [
    "## Load network (optional)\n",
    "To save time, you can skip from Step 1 to this step once you have trained the network once. This is not set up to work on Colab across sessions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aabdb1-480d-4cef-9453-dce0a325ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "if load_model:\n",
    "    # initialize things\n",
    "    print(\"Loading final model checkpoint\")\n",
    "    checkpoint_load_name = r\"resnet18_final.pth\" #\n",
    "    checkpoint_load_path = project_models_dir / checkpoint_load_name\n",
    "    device = 'cuda'\n",
    "    if checkpoint_load_path.exists() and torch.cuda.is_available():\n",
    "        print(\"model exists, cuda available.\")\n",
    "\n",
    "    # load the data\n",
    "    final_checkpoint = torch.load(checkpoint_load_path, weights_only=False)\n",
    "\n",
    "    # Unpack values you want (TODO: cut some of these you don't use)\n",
    "    trained_model = final_checkpoint['model']\n",
    "    optimizer = final_checkpoint['optimizer']\n",
    "    num_epochs = final_checkpoint['epochs']\n",
    "    train_loss = final_checkpoint['train_loss']\n",
    "    val_loss = final_checkpoint['val_loss']\n",
    "    train_topk_accuracy = final_checkpoint['train_accuracy']\n",
    "    val_topk_accuracy = final_checkpoint['val_accuracy']\n",
    "    topk = final_checkpoint['topk']\n",
    "else:\n",
    "    print(\"Not loading model -- likely a training run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07b4cfc-a46a-457f-a84e-04ea8b0c5345",
   "metadata": {},
   "source": [
    "# 8. Check model performance\n",
    "First let's visually inspect model performance over some random images from the validation data. Then we'll look at some metrics calculated over the entire validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82bdfe5-ea5a-48e6-8638-7adaefdca7cc",
   "metadata": {},
   "source": [
    "## Inspect some predictions\n",
    "We'll use a few deepglue convenience functions to get a few random images and predict their identity using the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2322b0aa-5860-444b-8641-ebb9593beeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_paths, rand_categories = dg.sample_random_images(data_dir, \n",
    "                                                      category_map, \n",
    "                                                      split_type='valid', \n",
    "                                                      num_images=10)\n",
    "random_stack = dg.load_images_for_model(rand_paths, \n",
    "                                        data_transform(train=False)); # nondistorting transform\n",
    "predicted_probs = dg.predict_batch(trained_model, random_stack, device=device); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2bbb12-d8bb-46df-a14b-6e3e4af51f2e",
   "metadata": {},
   "source": [
    "Using `dg.plot_prediction_grid()`, we'll plot the actual image and top prediction on the left, and the `top_n` predictions with their probabilities on the right: let's check out the top five predictions of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15649507-2f2b-470d-b997-7d40060a0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg.plot_prediction_grid(random_stack, \n",
    "                        predicted_probs, \n",
    "                        rand_categories, \n",
    "                        category_map, \n",
    "                        top_n=5, \n",
    "                        figsize_per_plot=(2, 2), \n",
    "                        logscale=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9bd491-0462-4e63-8ada-20c5fc10420e",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "While images are good (and we get good performance on individuals), we should examine performance over the entire validation data set.\n",
    "\n",
    "Scikit learn has many metrics we can use. We have already built a dataloader for validation data above, and deepglue has a function for predicting all the data given a dataloader and the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bb9efd-0395-4fd4-8979-60df95966abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds, val_labels, val_probs = dg.predict_all(trained_model, valid_data_loader, device=device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c992b-af0b-4e6d-a546-4ce8eef3ffa7",
   "metadata": {},
   "source": [
    "Now, with the predictions, correct labels, we can get lots of metrics. We'll focus on some of the basic metrics for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e1c4f-7e0c-469b-a59d-fe9416c12f23",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "The confusion matrix tells you the count of the actual category and predicted category for all ten categories, which reveals basic error patterns (the main diagonal shows when the network is correct).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fb29d3-eabd-426e-b97c-9ddef8633823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm_disp = ConfusionMatrixDisplay.from_predictions(val_labels,\n",
    "                                                  val_preds,\n",
    "                                                  display_labels=category_names,\n",
    "                                                  xticks_rotation=45.,\n",
    "                                                  cmap='magma');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa831dd7-ec91-4a00-8be4-656f29bad58b",
   "metadata": {},
   "source": [
    "### Classification report\n",
    "Scikit learn generates a useful classification report that tells you the precision (indicates levels of false positives along a column of the confusion matrix, off the main diagonal) and recall (indicates levels of false negatives along a row of the confusion matrix, off the main diagonal) for each category. F1 is a combination of both precision and recall.   \n",
    "\n",
    "The classification report also aggregates these measures into overall accuracy (overall proportion correct), average precision and recall (called 'macro average'), and weighted average (weighted by class size, which would be useful for imbalanced data). \n",
    "\n",
    "For more on the classification report, there is a useful discussion here: https://www.nb-data.com/p/breaking-down-the-classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd26b3-be34-48e1-9764-2398b375f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5722cd2e-3fd6-481a-8f14-92d7f3f624b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(val_labels, val_preds, target_names=category_names);\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df85c23-29c6-4450-b338-a3c6e93258e7",
   "metadata": {},
   "source": [
    "You can see that some of the categories are classified nearly perfectly (trousers, bags). While others are really tough for the classifier, in particular shirts (the recall is quite low, implying lots of false negatives -- images of shirts that were classified as non-shirts). Is this because our network needs more training, resnet18 is not up to the task, or maybe something intrinsic to our dataset? \n",
    "\n",
    "Before investing a ton of time tweaking parameters, let's do some visualization of our data and feature space to find the error patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffffb0ea-dabc-4b6d-b7f8-9ab819a1e968",
   "metadata": {},
   "source": [
    "# 9. Visualize clustering in feature space\n",
    "As just mentioned, while our network is performing very well, we have a decision to make. Should we tweak some parameters? Should we bring in some more fancy learning rate scheduler? Maybe train for more epochs? Some more heavy augmentation in our transformer might be helpful. Or maybe we should bring in a bigger network like `resnet50`.\n",
    "\n",
    "These are all reasonable options, but this notebook is meant to demo deepglue basics, not dive into the weeds of machine vision. Also, sometimes it is helpful to visualize what's happening inside of a network before spending that 80% of your time eking out that 2% improvement in the model performance. Pytorch provides some useful tools to for feature extraction from networks, and deepglue has utilities for visualizing the features embedded there. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b90f06-1b03-4c8a-8e27-d4a5a4901bb1",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "Feature extraction works by pushing images through the network, and extracting the activity patterns from the desired layers for those images.\n",
    "\n",
    "During normal network operation, we only care about the final prediction of the network: the network output. But for analysis of networks, `pytorch` provides a function (`create_feature_extractor()`) that we can use to extract activation values from *any layer* of the network when given images. You just feed the function a data loader, and it returns the activation values for the designatred layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3095ce-6388-45c2-bc25-05d186c6cd97",
   "metadata": {},
   "source": [
    "Toward that end, we will create a data loader specifically for the feature extractor: deepglue's `prepare_ordered_data()` creates a dataloader for feature extraction that will go in order through all the data in a data split. The function returns the data loader and the image paths (the latter will become important for visualizing the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf9bd0-3ba3-4e31-b701-b73af2c67cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths, ordered_loader = dg.prepare_ordered_data(data_dir, \n",
    "                                                      image_transforms['valid'], \n",
    "                                                      num_workers=num_workers, \n",
    "                                                      batch_size=56, \n",
    "                                                      split_type='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f64264-3538-4e46-b9aa-a51c84dec04b",
   "metadata": {},
   "source": [
    "Next, we have to decide which layers we want to extract features from. \n",
    "\n",
    "In torchvision, the convention is that the variable `return_nodes` defines the layers from which features can subsequently be extracted (keys are the actual layer names, and values are the names we give them for access later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9ac594-5846-47b3-af50-5d5ff8a4a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_nodes = {'relu': 'cnn_features',                  # Early traditional CNN features\n",
    "                'layer1.1.relu': 'resnetL1_features',\n",
    "                'layer2.1.relu': 'resnetL2_features',\n",
    "                'layer3.1.relu': 'resnetL3_features',\n",
    "                'layer4.1.relu': 'resnetL4_features'}\n",
    "pprint(return_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1fde8f-2ad5-4da9-8f8c-da0b0ca87b06",
   "metadata": {},
   "source": [
    "To list all of the options for extracting features you could run the following code (expect a long printout):\n",
    "\n",
    "    for name, module in trained_model.named_modules():\n",
    "        print(name)\n",
    "\n",
    "We are going to keep it simple and just examine the activity very late to visualize clustering of higher level abstract features. Note if you were using `resnet50`, there would be *many* more layers to choose from.\n",
    "\n",
    "Note the earliest one (relu) is from the first convolutional layer that doesn't include any residual blocks. It would provide access to very low-level features, but beware there would be *lots* of dimensions so you would needs lots of RAM. The other four are the outputs of the four residual blocks (we expect higher-level fashion features in layer 4 so will extract those in what follows).\n",
    "\n",
    "Torchvision's `create_feature_extractor()` just needs our trained model and dictionary of return nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8be5ec-652a-462a-af61-df39be2c2bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.eval();\n",
    "feature_extractor = create_feature_extractor(trained_model.to(device), \n",
    "                                             return_nodes=return_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46071b8f-4aa5-462b-83f6-996424a24c18",
   "metadata": {},
   "source": [
    "Things are all set up. We can pick the layer (or layers) we want, and run feature extraction. This can take a while because it is running through the entire validation dataset as it extracts the activation patterns from the selected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b20e5-5d07-4810-8b92-389dcf3d242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'resnetL4_features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e448dee6-73e7-49bc-a9de-33534412612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = dg.extract_features(ordered_loader, \n",
    "                                       feature_extractor, \n",
    "                                       layer=layer,\n",
    "                                       device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d73f1-6256-4365-9648-a5802d628da8",
   "metadata": {},
   "source": [
    "The features extracted are fairly high dimension (over 25k dimensions). This is actually much lower than if we had extracted features from earlier layers in the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0ce80b-45bf-4d37-89e8-af067535d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7fa759-38b4-499c-86e0-3a9e5790f96d",
   "metadata": {},
   "source": [
    "## Dimensionality reduction and visualization\n",
    "It is hard to visualize tens of thousands of features. We will now use umap to project these high-dimensional activity patterns to a 2d space to visualize how much they are clustering according to category. We will visualize this using a static plot (Matplotlib) and an interactive plot (using Bokeh).\n",
    "\n",
    "The following umap-baed visualizations are adapted from a umap demo: https://umap-learn.readthedocs.io/en/latest/basic_usage.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2482e021-f644-4a59-86fd-c7a9c9c8920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = umap.UMAP(n_components=2, \n",
    "                       n_epochs=200, \n",
    "                       low_memory=True,\n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c7482-68be-44c0-88d8-cb2e7c79ceb7",
   "metadata": {},
   "source": [
    "UMAP is itself an iterative ML algorithm, so the following can take a minute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13766c08-f12d-4396-84bf-3e6db24f8122",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "layer_features_umap = umap_model.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b111fa1-6475-4ada-ad39-d56fc4dd0ae7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3 style=\"margin: 1px 0 6px 0;\">Other dimensionality reduction techniques</h3>\n",
    "\n",
    "If you want to compare UMAP to PCA or other dimensionality reduction techniques, it should flow nicely through in the code. For instance:    \n",
    "<code>\n",
    "pca = PCA(n_components=2)\n",
    "layer_features_pca = pca.fit_transform(features)\n",
    "</code>\n",
    "    \n",
    "You can then just replace `layer_features_umap` with `layer_features_pca` in the code. PCA is much faster, and will project based just on variance which is easier to interpret (the clusters won't look as nice though). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f5be3c-e32d-44f7-9720-1f80f11cdf84",
   "metadata": {},
   "source": [
    "### Static visualization\n",
    "Let's first do a static visualization of the feature clusters using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a41a1-a6dc-4f45-a544-26318c923aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Scatter plot of the subsampled features\n",
    "scatter = plt.scatter(\n",
    "    layer_features_umap[:, 0],  # x-coordinates from UMAP\n",
    "    layer_features_umap[:, 1],  # y-coordinates from UMAP\n",
    "    c=labels,           # Color code by labels\n",
    "    cmap=plt.cm.tab10,              \n",
    "    s=10,                          \n",
    "    alpha=0.7                      \n",
    ")\n",
    "\n",
    "# Adding a colorbar for the labels\n",
    "colorbar = plt.colorbar(scatter, boundaries=np.arange(num_classes + 1) - 0.5)\n",
    "colorbar.set_ticks(np.arange(num_classes))\n",
    "colorbar.set_ticklabels(category_names)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "# Title and labels\n",
    "plt.title('UMAP Projection of Fashion')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "\n",
    "# Show plot\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e119b4-a384-4506-8721-45db9717be7b",
   "metadata": {},
   "source": [
    "We can see patterns that match what we saw with our classification report: trousers and bags clearly distinct. Shirts (in pink) are all mixed in with other categories: the network just hasn't learned to separate out these categorical features. However, it would be nice to have more details. For instance, which of those spots were correct/incorrect? What do the shirts actually look like that are close to t-shirts? Could *we* differentiate them? Let's build an interactive visualization tool that will give us such details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0036f69-002c-4fdd-9d57-a00a07bd4ed3",
   "metadata": {},
   "source": [
    "### Interactive feature visualization\n",
    "We will build an interactive scatter plot that plots a little embeddable image sprite when we hover over a point in the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5d572-400c-4bb0-817e-35ed5a118e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_to_embed = 7\n",
    "encoded_image_str = dg.create_embeddable_image(image_paths[ind_to_embed], size=(60,60))\n",
    "HTML(f'<img src=\"{encoded_image_str}\" />')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8fc2ae-2c17-4e30-a718-c889c0c96de5",
   "metadata": {},
   "source": [
    "Is the above a shirt or a tshirt? What criteria were used for these categories anyway? These are things we can dig into a bit in what follows. \n",
    "\n",
    "To get predictions that map onto the ordered data loader that we used for feature extraction, we need extract predictions from the ordered data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b5a03-fef9-46ae-9718-532eceee58f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_predictions, ordered_labels, _ = dg.predict_all(trained_model, ordered_loader, device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41f7e21-435a-4dce-8d0d-35e7711b6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BTW, the above was a {category_names[ordered_labels[ind_to_embed]]}\")\n",
    "print(\"How did the network do?\")\n",
    "print(f\"Actual category: {ordered_labels[ind_to_embed]}: Predicted category {ordered_predictions[ind_to_embed]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f33298-2abd-41ca-90f7-72b77de270e9",
   "metadata": {},
   "source": [
    "We can use deepglue's `plot_interactive_projection()` to plot everything we wanted in one plot:\n",
    "- Scatter plot of the features projected in umap space, where hovering shows you see the sprite of the corresponding image.\n",
    "- o's show correct predictions, x's show incorrect predictions so you can see what features confused the network.\n",
    "- You can interact with the plot by zooming, selecting regions that you are interested in (see selection tools on right to determine whether you wheel zoom or box select)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdaafdd-7472-4a51-91cb-e04172c6027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg.plot_interactive_projection(layer_features_umap, \n",
    "                               ordered_labels.numpy(), \n",
    "                               image_paths, # to show sprites\n",
    "                               category_map,\n",
    "                               predictions=ordered_predictions.numpy(), # to differentiate correct/incorrect\n",
    "                               title='UMAP Projection', \n",
    "                               image_size=(75, 75), # size of sprite popups\n",
    "                               plot_size=500, # x/y dims of plot\n",
    "                               legend_location=\"top_left\",\n",
    "                               show_in_notebook=True) # if not True, will pop up plot in new tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d21443-f09a-4ad5-8367-9c3c37fc87b5",
   "metadata": {},
   "source": [
    "With this visualization we can start to see the range of visual features that are important for each category, and how they start to blend into each other and make category membership determination very hard (even for us). This kind of information can be very helpful for making decisions about what direction to take a project. We also see what differentiates tshirts from shirts (canonical shirts have buttons, tshirts do not, though these distinctions are not perfectly honored in this messy real-world data set). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b554c1-6972-491f-999e-fe5e4d964c3c",
   "metadata": {},
   "source": [
    "# 10. Test data\n",
    "\n",
    "So far, we have purposely held off analyzing the test data. We really want to avoid data leakage, so we purposely keep the test data sealed off until we are sure we have the final comparisons we want to make. \n",
    "\n",
    "Once you have put your final candidate model (or models) through the ringer, and narrowed things down to a small set of models (e.g., your best resnet, vgg, whatever, with the best augmentation and learning rate schedulers), then it's time to put them to the final test with your test data to see which generalizes best outside of all of our training regime (which includes training and validation splits). At that point it's time to get an estimate of how well the model generalizes outside *all* of the training data. It probably won't be as good as for the validation data, which we used to tweak the model multiple times. That's why we set aside the test data and leave it in a lock box until the very end. \n",
    "\n",
    "I'm purposely leaving it out of this notebook to give people a chance to try different models (e.g., `resnet50`), add different augmentations, tweak things, and add whatever improvements to the model(s). You can then apply the same metrics we went over above with validation data (accuracy, recall, etc) to the test data and see how well things generalized. \n",
    "\n",
    "This is left as an exercise for the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c68f5f-a4f2-4765-b747-6ab8356bf0ed",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3 style=\"margin: 1px 0 6px 0;\">What about fashion mnist?</h3>\n",
    "\n",
    "For those that want to use the above tools using the full fashion_mnist dataset with all 70,000 images, it is available at the following link:\n",
    "\n",
    "<code>full_data_url = r'https://drive.google.com/uc?id=1B15ViE9lKquepM2TCvApe9gGPeQIxiUS'</code>\n",
    "\n",
    "You will have to adjust some of the above variable names from Section 2 of this notebook (e.g., `fashion_3k` will become `fashion_mnist` or something like that), but this will be a good exercise in working with multiple datasets within a project. \n",
    "\n",
    "Using the full dataset will yield much better metrics on your classifier.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e9e34a-cba8-403e-abb2-191f3c0a910f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
