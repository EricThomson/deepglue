{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c234dd0e-abf7-4b35-b565-aaa8a28fc346",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a8f7e-1db0-4c46-83a5-e1a3f2c331f9",
   "metadata": {},
   "source": [
    "# 0. Intro: deepglue demo\n",
    "This demo will highlight some of the features of deepglue using the [fashion mnist](https://github.com/zalandoresearch/fashion-mnist) dataset for classification. It includes 70k pictures of different articles of clothing sorted into ten categories (e.g., t-shirts). \n",
    "\n",
    "Let's import libraries and set up a logger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7df6e3-5315-406d-a2c6-f5be8db1aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "from IPython.display import HTML\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torchvision.transforms.v2 import ToImage, ToDtype\n",
    "\n",
    "import umap\n",
    "import zipfile\n",
    "\n",
    "import deepglue as dg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71227e4-75ef-4ba2-9afe-5dbabb7f6670",
   "metadata": {},
   "source": [
    "We'll set the logger to just print warnings for now. If you run into problems you can set the logging level to `DEBUG`.\n",
    "\n",
    "If you don't want your world clutter with logging, you can delete the following or comment it out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79384ef2-0e40-493d-b00a-74349f5cd133",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_format = \"%(asctime)s - %(filename)s - %(funcName)s - line %(lineno)d - %(levelname)s - %(message)s\"\n",
    "logging.basicConfig(level=logging.WARNING,  # You can set this to the desired logging level INFO DEBUG WARNING etc\n",
    "                    format=log_format)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b061539-7f05-4740-8246-6316fbca2927",
   "metadata": {},
   "source": [
    "In what follows, you can explore any function's documentation by typing the function name followed by a question mark. E.g.,  just type in `dg.validate_one_epoch?` in a code cell to get a printout of the documentation for `dg.validate_one_epoch()`. To get a deeper dive into a deepglue function, you can go to the source code at [the repo](https://github.com/EricThomson/deepglue) or the [online documentation](https://deepglue.readthedocs.io/en/latest/). For instance, you can explore how `validate_one_epoch()` works at the [online documentation for the function](https://deepglue.readthedocs.io/en/latest/api/#deepglue.training_utils.validate_one_epoch). Also, you can ask questions at the repo's [Discussion forum](https://github.com/EricThomson/deepglue/discussions). If you find a problem, please [raise an issue](https://github.com/EricThomson/deepglue/issues). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48b7d55-9768-495d-b1ef-1449c4783bba",
   "metadata": {},
   "source": [
    "# 1. Set up directory structure \n",
    "It's key to be organized for any deep learning project. In what follows we will set up a directory structure for the project. \n",
    "\n",
    "## Set up directory for all projects\n",
    "Since this is the first demo, we'll set it up the default projects directory where all deepglue projects will be stored. \n",
    "\n",
    "The following will set up projects in `~/.deepglue/projects/` (Linux/Mac) or `C:/Users/Username/.deepglue/projects/` (Windows). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548ed78-6995-4140-8046-639f2b0e0e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change the following defaults, this is just for convenience to get things started.\n",
    "deepglue_dir = Path.home() / \".deepglue\"\n",
    "projects_dir = deepglue_dir / \"projects\"\n",
    "\n",
    "try:\n",
    "    projects_dir.mkdir(parents=True, exist_ok=False) # prevent overwriting\n",
    "except FileExistsError:\n",
    "    print(\"'projects/' directory already exists. Skipping creation.\\n\")\n",
    "\n",
    "print(f\"Your deepglue projects directory is: {projects_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96347a9b-1f62-4f64-8cfb-bb8e13c71a7a",
   "metadata": {},
   "source": [
    "## Directory for fashion project\n",
    "Now let's create the fashion project directory and minimal structure for our fashion mnist classification project. Given a new project name (`fashion`), deepglue's `create_project()` function creates the following directory structure within `projects/`:\n",
    "\n",
    "    fashion/\n",
    "        data/\n",
    "        models/\n",
    "\n",
    "This is a kind of minimal structure for a deep learning project. You can add more as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccb239f-47d3-4828-a118-5dc72239c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"fashion\"\n",
    "project_dir, project_data_dir, project_models_dir = dg.create_project(projects_dir, project_name)\n",
    "\n",
    "print(f\"Your new project directory: {project_dir}\")\n",
    "print(f\"Datasets will go into {project_data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6fae0-e08b-4d21-a250-eb977c6e6f94",
   "metadata": {},
   "source": [
    "# 2. Download data\n",
    "Our slightly modified fashion-mnist dataset (modified to be rgb just to make some things simpler), is in the cloud in a zip file. Let's download and unzip the data into the `project_data_dir` we already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaafa11-f9ea-472c-9c54-e6f8d2c6a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = r'https://drive.google.com/uc?id=1B15ViE9lKquepM2TCvApe9gGPeQIxiUS'\n",
    "filename = 'fashion_mnist.zip'\n",
    "data_zip_path = project_data_dir / filename\n",
    "data_dir = project_data_dir / 'fashion_mnist' # root data dir for project\n",
    "print(f\"Will attempt download to {data_zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a6f1a-e45d-41a2-9b54-04ceb327f5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not data_zip_path.exists():\n",
    "    print(\"Downloading...may take few seconds\")\n",
    "    gdown.download(data_url, str(data_zip_path), quiet=True, fuzzy=True)\n",
    "else:\n",
    "    print(f\"{filename} already downloaded. Download skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a45050c-22c2-49e1-a92b-bf16f3df9efa",
   "metadata": {},
   "source": [
    "Let's unzip the compressed data using zipfile's `extractall()`. The following has some extra wrinkles to avoid errors and repeating decompression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db6be9f-39d3-4260-af1a-adf3d14df5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if data_dir.exists():\n",
    "    print(\"Already extracted. Skipping.\")\n",
    "else:\n",
    "    if data_zip_path.suffix == '.zip':\n",
    "        print(f\"Unzipping to {project_data_dir}\\nLots of files, so it takes a while...\")\n",
    "        with zipfile.ZipFile(data_zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(project_data_dir)\n",
    "        print(\"Done unzipping!\\n\")\n",
    "    else:\n",
    "        print('Not a zip file.')\n",
    "\n",
    "train_dir = data_dir / 'train'\n",
    "valid_dir = data_dir / 'valid'\n",
    "test_dir = data_dir / 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d2e97b-1203-4b53-86f9-ca5235f04705",
   "metadata": {},
   "source": [
    "Our final directory structure follows a standard that is used by several functions in `pytorch`:\n",
    "\n",
    "        projects/\n",
    "            fashion/\n",
    "                models/\n",
    "                data/\n",
    "                    fashion_mnist/\n",
    "                        train/\n",
    "                            0/  [tshirt]\n",
    "                            1/  [trouser]\n",
    "                        valid/\n",
    "                            0/\n",
    "                            1/   \n",
    "                        test/\n",
    "                            0/\n",
    "                            1/\n",
    "\n",
    "Each end node in this directory tree (`0/`) contains image data from the relevant category: feel free to navigate to the relevant spots on your machine to check out the data. Below we will show how to use `deepglue` to inspect random samples of the data. \n",
    "\n",
    "Our mapping from subdirectory names to actual categories is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef936f98-2143-448a-955b-f14993a412c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map = {'0': 'tshirt',\n",
    "                '1': 'trouser',\n",
    "                '2': 'pullover',\n",
    "                '3': 'dress',\n",
    "                '4': 'coat',\n",
    "                '5': 'sandal',\n",
    "                '6': 'shirt',\n",
    "                '7': 'sneaker',\n",
    "                '8': 'bag',\n",
    "                '9': 'ankle_boot'}\n",
    "\n",
    "categories = ['0','1','2','3','4','5','6','7','8','9']\n",
    "category_names = [category_map[key] for key in categories]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c621603-dc1f-442e-8892-66603d3f92a4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3 style=\"margin: 1px 0 6px 0;\">Scalable data structures</h3>\n",
    "In real-world projects there will often be multiple datasets placed in the project's top-level <code>data/</code> directory. Since we are working with a single dataset, we could in theory just extract the <code>train/</code>, <code>valid/</code> and <code>test/</code> folders directly into <code>data/</code> to keep a more flat structure. But our more nested structure mimics how larger projects handle multiple datasets in a scalable way, so we'll stick with it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f62e8b-4e9d-438e-9c7c-3a31bee3a4e2",
   "metadata": {},
   "source": [
    "# 3. Explore data \n",
    "An initial look at the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a87d1-bfdd-4f05-b16e-ce22798f4136",
   "metadata": {},
   "source": [
    "Deepglue will plot some random images from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bd38a2-a242-4b2f-84b6-033fb606fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg.plot_random_sample(data_dir, category_map, split_type='train', num_to_plot=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d15f51e-0150-4bb1-9472-dd4bcb63f29e",
   "metadata": {},
   "source": [
    "Within the project, the data are divided into training, validation, and testing splits. How many are in each split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a2242-58db-4c2b-8de7-d4d323d413b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_per_split = dg.count_by_split(data_dir)\n",
    "\n",
    "total_samples = num_per_split['train'] + num_per_split['valid'] + num_per_split['test']\n",
    "print(f\"Num samples total: {total_samples}\")\n",
    "pprint(num_per_split)\n",
    "data_splits = ['train', 'valid', 'test']\n",
    "proportion_per_split =  [num_per_split['train']/total_samples, \n",
    "                         num_per_split['valid']/total_samples, \n",
    "                         num_per_split['test']/total_samples]\n",
    "\n",
    "# Plot number in train/validation/test splits\n",
    "plt.bar(data_splits, proportion_per_split)\n",
    "plt.title(\"Proportion per split\")\n",
    "plt.xlabel('Split Type')\n",
    "plt.ylabel('Proportion');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79078ca-2ffb-401b-a491-3cb778127cd6",
   "metadata": {},
   "source": [
    "We have 70k total samples, with 50k set aside for training, and 10k for validation and testing respectively (70%, 15%, 15%). We can check out how many are in each category, within each split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f427ce0-eaef-4dd4-8c58-49165d804c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_category_by_split = dg.count_category_by_split(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90529816-7472-4bd3-84dc-a3a95d8466fc",
   "metadata": {},
   "source": [
    "That creates a dict with the splits as the keys (`train`, `valid`, and `test`), and each contains the number in each category in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8f574-e23d-4a43-9955-d55d04ed26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training split counts:')\n",
    "pprint(num_category_by_split['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03000b90-6146-41ee-9cfb-5e9f9cf07fcc",
   "metadata": {},
   "source": [
    "Let's plot the proportion represented in the categories in the three data splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9851e7a-0256-4a92-8911-7fcc975a44e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counts and proportions\n",
    "train_counts = np.array([num_category_by_split['train'][key] for key in categories])\n",
    "train_proportions = train_counts/sum(train_counts)\n",
    "\n",
    "valid_counts = np.array([num_category_by_split['valid'][key] for key in categories])\n",
    "valid_proportions = valid_counts/sum(valid_counts)\n",
    "\n",
    "test_counts = np.array([num_category_by_split['test'][key] for key in categories])\n",
    "test_proportions = test_counts/sum(test_counts)\n",
    "\n",
    "# plot them\n",
    "fig, (ax_test, ax_val, ax_train) = plt.subplots(3,1,figsize=(5,5)) # width x height\n",
    "\n",
    "#train (bottom)\n",
    "ax_train.bar(category_names, train_proportions)\n",
    "ax_train.tick_params(axis='x', labelrotation=45)\n",
    "ax_train.set_title(\"Training Data\")\n",
    "ax_train.set_xlabel('Category')\n",
    "\n",
    "# validation (middle)\n",
    "ax_val.bar(category_names, valid_proportions)\n",
    "ax_val.set_ylabel('Proportion');\n",
    "ax_val.set_title(\"Validation Data\")\n",
    "ax_val.set_xticks([]) \n",
    "\n",
    "# test (top)\n",
    "ax_test.bar(category_names, test_proportions)\n",
    "ax_test.set_title(\"Test Data\")\n",
    "ax_test.set_xticks([])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cdea37-d23c-45df-9c39-0bf8d4291447",
   "metadata": {},
   "source": [
    "We can see this is (by design) an extremely balanced data set, in all three splits. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24308eac-4643-4251-8790-66b93493a40e",
   "metadata": {},
   "source": [
    "# 4. Define network model\n",
    "To learn the fashion classificaiton task, we'll start with a pre-trained [resnet18 model](https://www.run.ai/guides/deep-learning-for-computer-vision/pytorch-resnet) that was trained on the Imagenet 1k dataset (1000 categories with over 1 million total images). It's a relatively simple pre-trained model to use as a base for transfer learning. We are initially freezing all the model parameters, and then will unfreeze the final two convolution layers, and replace the fully connected (fc) layer. So we'll be keeping the early layers which extract basic features, but allowing the later layers to learn the higher-level fashion-specific features in fashion-mnist. \n",
    "\n",
    "Feel free to substitute a different model and adapt the code to suit your needs. For instance, `resnet50` will almost certainly perform better, but it will also take up way more memory and take longer to train. So for this little demo, we're sticking with `resnet18` (which also performs very well). We'll discuss the topic of trying out different models at the end in the `Test data` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d1c3c6-2fa0-4eb6-830e-ab530921ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(category_map)  # Adjust to match the number of classes in Fashion MNIST\n",
    "num_fc_hidden_units = 256  # Number of hidden units in fully connected layer: feel free to tweak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e17b94a-83e0-4590-a1ff-a1997c21892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet50 model\n",
    "resnet_weights = models.ResNet18_Weights.IMAGENET1K_V1\n",
    "resnet18 = models.resnet18(weights=resnet_weights)\n",
    "\n",
    "# Freeze all model parameters initially\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# unfreeze blocks 3 and 4\n",
    "for param in resnet18.layer3.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "for param in resnet18.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Get the number of inputs for the original fully connected (fc) layer\n",
    "num_fc_inputs = resnet18.fc.in_features\n",
    "\n",
    "# Replace the final fully connected layer (note requires_grad is True by default for a brand-new layer)\n",
    "resnet18.fc = nn.Sequential(nn.Linear(num_fc_inputs, num_fc_hidden_units),  # Projection from backbone to hidden units\n",
    "                            nn.ReLU(),                                      # Activation for non-linearity\n",
    "                            nn.Linear(num_fc_hidden_units, num_classes))    # Final layer for class prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6441f7c5-fb9b-428c-ab4a-2d905691253c",
   "metadata": {},
   "source": [
    "We'll also define the loss function and optimizer (which includes the learning rate schedule) for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a9ba93-013b-467a-857b-6bcd149b9d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(resnet18.parameters(), lr=0.0001)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2e8f6e-6c4f-4e7d-9b02-520881044d56",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3 style=\"margin: 1px 0 6px 0;\">Expected inputs for resnet</h3>\n",
    "Resnet models were trained on 224 x 224 images, and this matters. If you try to feed the model different sized images, it will not perform as well, or you may get errors at some point in your project. Fashion mnist images are 28x28, so in what follows we will upsample them to 224x224. We'll also convert the inputs to RGB. We could define a completely new network and train from scratch, but then we'd lose out on the gains from the pretrained network. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5170480-66c2-4db2-918c-1062e857cdf3",
   "metadata": {},
   "source": [
    "# 5. Set up data for training\n",
    "Having a bunch of data in folders is great, but `torchvision` provides lots of utilities to make funneling such data through training pipelines really easy. Also, integrating transformations like random cropping, rotations, and normalization directly into the our pipelines simplifies training, and `torchvision` has a great api that integrates such augmentations directly into the data pipeline. We'll start there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93818e02-925c-40ce-8ac1-66f8fd2fc6c4",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "Torchvision has an amazing set of transforms that apply whether you are doing classification, object detection, or scene segmentation. To learn more about the transforms, see their [documentation](https://pytorch.org/vision/main/transforms.html#v2-api-ref) or [example page](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py). \n",
    "\n",
    "We are going to set up a relatively simple transform here just to show the logic. First we'll define a couple of transforms that we will randomly apply in our transform function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a43bfd0-9111-43d3-9206-7fd79143a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_noise = transforms.GaussianNoise(mean=0, # mean of sampled noise\n",
    "                                         sigma=0.1, # std of sampled noise\n",
    "                                         clip=True)  # clip to [0,1] after adding noise\n",
    "gaussian_blur = transforms.GaussianBlur(kernel_size=(7,7), # kernel size (width, height)\n",
    "                                        sigma=(0.8, 0.8)) # sigma: min, max randomly chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f94c68-88ac-4133-acb4-b33ae0b561e1",
   "metadata": {},
   "source": [
    "Then define a function for a data transform that can be applied to different data splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a82c3c-b5bd-45da-8be1-fd956e257418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transform(train=False):\n",
    "    \"\"\"\n",
    "    Define data transformation to be used when data is ingested for use by datasets.\n",
    "    \"\"\"\n",
    "    transform_pipeline = []\n",
    "    \n",
    "    transform_pipeline.append(transforms.Resize((224, 224)))     # resnet expect this size\n",
    "    transform_pipeline.append(transforms.ToImage())  # Convert PIL image to tensor: many transforms only work on torch tensor\n",
    "    transform_pipeline.append(transforms.ToDtype(torch.float32, scale=True)) # many transforms only work with float\n",
    "    transform_pipeline.append(transforms.RGB()) # resnet expects RGB (if it is already RGB, nothing changes)\n",
    "\n",
    "    if train:\n",
    "        transform_pipeline.append(transforms.RandomApply([gaussian_noise], p=1/2)) # will apply p of the time\n",
    "        transform_pipeline.append(transforms.RandomApply([gaussian_blur], p=1/2))  \n",
    "        transform_pipeline.append(transforms.RandomHorizontalFlip(p=1/2))\n",
    "        transform_pipeline.append(transforms.RandomRotation(20, fill=0.445, expand=False))  # will rotate (-20,20); expand would resize so image fits in image shape\n",
    "        \n",
    "    # Normalize to standard values for resnet\n",
    "    transform_pipeline.append(transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                   std=[0.229, 0.224, 0.225]))  \n",
    "    return transforms.Compose(transform_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f1ff68-5efe-422a-a9c8-5651c5b52170",
   "metadata": {},
   "source": [
    "In the above, we have defined a general transform function that we can define for diffrent data splits: those that are set for training data will have distortions applied to the data for augmentation purposes (gaussian blur, noise, flips, rotations). Those for validation and test will only have \"nondistorting\" changes applied: they will be rescaled to the proper size, changed to torch tensors, converted to floats, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4523831-ddc0-4a3c-ab5b-bf4686b06b44",
   "metadata": {},
   "source": [
    "### Transform demo\n",
    "Let's look at how the transform works in the case of training data just for fun. We'll set train to `True` so we'll see the effects of the augmenting transforms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144d3a8-91bf-4488-837b-a83d172e7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transform = data_transform(train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7656affe-80a3-4536-a104-b2d75d683e23",
   "metadata": {},
   "source": [
    "Let's get a random image from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d9b06-e3d7-4d06-afb2-3ec00a11127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_plot = '7'\n",
    "demo_image_path, demo_image_category = dg.sample_random_images(data_dir, \n",
    "                                                               category_map, \n",
    "                                                               category=category_to_plot) \n",
    "demo_image = np.array(Image.open(demo_image_path[0]))\n",
    "dg.plot_transformed(demo_image, demo_transform, num_to_plot=9);\n",
    "print(category_map[category_to_plot])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7669f1d7-61fe-41f9-b378-ffd1fef552ef",
   "metadata": {},
   "source": [
    "You can see that we get multiple views of the sneaker, some are pretty distorted. This is good! We want to give the network some tricky instances for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13af67fc-e89d-4c61-86d2-4758dd4aa028",
   "metadata": {},
   "source": [
    "### Define transforms for our datasets\n",
    "Let's define three different image transforms that will be used by our dataset pipeline when we load data from disk: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc86b3fd-29c3-4a88-ae6b-231f786156ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = {'train': data_transform(train=True),\n",
    "                    'valid': data_transform(train=False),\n",
    "                    'test': data_transform(train=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6a89e-be6c-4337-83e8-b6ac17730800",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "Let's create our training, validation, and test datasets using torchvision's `datasets.ImageFolder` class. This is a convenience class that hooks into image folders that are structured the way we have set them up. In general, pytoch datasets provide a flexible way to define how your data is loaded, transformed, and accessed, making it highly useful for deep learning workflows. The primary use of datasets is get wrapped into a `DataLoader`, which will handle batching, shuffling, and parallel data loading using multiple workers. Datasets are designed to load data lazily, meaning they load individual items on-the-fly when accessed: this minimizes memory usage, which is especially helpful fo large datasets. \n",
    "\n",
    "We'll create a dictionary of datasets, one dataset for each data split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d465027-6a12-4995-b3c3-1d3539d82419",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'train': datasets.ImageFolder(root=train_dir, transform=image_transforms['train']),\n",
    "        'valid': datasets.ImageFolder(root=valid_dir, transform=image_transforms['valid']),\n",
    "        'test': datasets.ImageFolder(root=test_dir, transform=image_transforms['test'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8974bc-33a6-4b4a-bc7e-9e49d3c36a92",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "The point of the above is to create data loaders, which is what we actually use directly during training. They shuffle the data, create batches, and parallelize things with multiprocessing to make everything go faster. \n",
    "\n",
    "Dataloaders require a couple of parameters and you might have to adjust these depending on your system. Just to review some ML terminology: one *epoch* is a single run through the entire data set. Our training data has 50k images, so one epoch of the training data will be a run through 50k images. This is way too much to run through all at once. A *batch* is the number of images in a subset that is run for processing in each loop during training/validation. The error is calculated for this subset and the network is updated during training for each batch. `num_workers` is the number of (CPU) processes that will work to generate batches in parallel: it can significantly speed up runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b3020-a31f-42be-9de6-cab290aaf8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256   # 128 on laptop, 256 on workstation\n",
    "num_workers = 6   # 4 on laptop, 6 on workstation\n",
    "\n",
    "# persistent workers makes things startup faster, but only if you have multiple workers\n",
    "if num_workers > 0:\n",
    "    persist_workers = True\n",
    "else: \n",
    "    persist_workers = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a401465-0a31-4f3a-aee7-d7b87ec8cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(data['train'], \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=True,\n",
    "                               num_workers=num_workers,\n",
    "                               drop_last=True,  # drop dangling batch at end\n",
    "                               persistent_workers=persist_workers)\n",
    "\n",
    "valid_data_loader = DataLoader(data['valid'], \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=True,\n",
    "                               num_workers=num_workers,\n",
    "                               persistent_workers=persist_workers)\n",
    "\n",
    "test_data_loader = DataLoader(data['test'], \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True,\n",
    "                              num_workers=num_workers,\n",
    "                              persistent_workers=persist_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6432293-4e07-4a47-8401-7ea9073934f7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3 style=\"margin: 1px 0 6px 0;\">Multiprocessing on different operating systems</h3>\n",
    "Different systems can take extra time to create multiple workers. Windows and (newer) Macs uses a different method than Linux to create new workers. On some older Windows systems, setting <code>num_workers</code> to any number greater than <code>0</code> will simply cause your system to hang. If this happens, you unfortunately have to set <code>num_workers = 0</code>. However, do give it a minute to start, because once the workers are set up initially, the speed payoff is <em>significant</em>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74610d36-eca7-4019-b541-5f0005b04c98",
   "metadata": {},
   "source": [
    "If you have already trained a model, you can jump to step 7 -- load the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef90106-aa78-42ad-a224-bc82a3b917bf",
   "metadata": {},
   "source": [
    "# 6. Train the network\n",
    "We'll use deepglue's `train_and_validate()` to run through the training data (50k images) and validation data (10k images). \n",
    "\n",
    "There are a couple of parameters here to set. How many times will we cycle throught the datasets during training (`num_epochs`), and the `topk` accuracy values to return. Top-k prediction counts as correct if the prediction was in the top k highest probabilities from the network (e.g., if the network's highest estimates for an image were `[sneaker, sandal, ankle_boot]`, for a `sandal` then it was top-3 accurate). This is useful for datasets with lots of similar categories. \n",
    "\n",
    "We are training on four epochs because initial exploration showed validation accuracy started to dip around this point (overfitting), and we want this demo to not take too long to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4420a2-30ae-4618-ab64-4c1eee96c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = (1,3)\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa2ca9a-5b4c-4b78-852b-30dc1a644d15",
   "metadata": {},
   "source": [
    "Note the following can take a long time (many minutes), especially on a laptop. As mentioned above, it can take a while to get started as it sets up the workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0acc6c7-0a99-47e8-907d-49678927a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  reset our logging level so we can get some feedback during training\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "trained_model, train_history = dg.train_and_validate(resnet18,\n",
    "                                                     train_data_loader,\n",
    "                                                     valid_data_loader,\n",
    "                                                     loss_func,\n",
    "                                                     optimizer,\n",
    "                                                     device='cuda',\n",
    "                                                     topk=topk,\n",
    "                                                     epochs=num_epochs);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d4d780-45a4-49cb-b077-ae93debf4575",
   "metadata": {},
   "source": [
    "Under the hood, `dg.train_and_validate()` works by calling `dg.train_one_epoch()` and `dg.validate_one_epoch()`, each `num_epochs` times.  `dg.train_one_epoch()` is the main workhorse for training models. It takes in the model, data loader (and a few other things), and steps through training for the required number of batches to train through all the data for an entire epoch. If you want to understand how that works, it can be instructive to step through the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feab372-1cb0-4263-bbd9-fdcab4e1cc1e",
   "metadata": {},
   "source": [
    "If you run out of GPU memory, try decreasing batch size and number of workers. On a laptop with 4GB of GPU RAM, 8 CPUs, the above cell took about 15 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff6f2d4-d0c2-4b51-81ec-12f22f191d2d",
   "metadata": {},
   "source": [
    "## View loss/accuracy\n",
    "Let's see how it did during training. Note we will explore other metrics below this is just to get a quick sense for how things went. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b5a18-7042-4f25-aed7-5c6dc8efaf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_array = np.arange(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a0535-fd0b-478a-8e6a-16d20db6ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(10,4))\n",
    "\n",
    "# Plot loss \n",
    "ax_loss.plot(epoch_array, train_history['train_loss'], color='blue',  label=\"Training Data\", marker='.')\n",
    "ax_loss.plot(epoch_array, train_history['val_loss'], color='firebrick', label=\"Validation Data\", marker='.');\n",
    "ax_loss.set_title('Training Loss')\n",
    "ax_loss.set_ylabel('Loss')\n",
    "ax_loss.set_xlabel('Epoch')\n",
    "ax_loss.legend();\n",
    "\n",
    "# Plot topk accuracies\n",
    "ax_acc.plot(epoch_array, train_history['val_topk_accuracy'][:,1], color='firebrick',  label=\"Validation top 3\", marker='.');\n",
    "ax_acc.plot(epoch_array, train_history['train_topk_accuracy'][:,1], color='blue', label=\"Training top 3\", marker='.')\n",
    "ax_acc.plot(epoch_array, train_history['val_topk_accuracy'][:,0],  color='darksalmon', label=\"Validation top 1\", marker='.');\n",
    "ax_acc.plot(epoch_array, train_history['train_topk_accuracy'][:,0],  color='lightsteelblue', label=\"Training top 1\", marker='.')\n",
    "\n",
    "ax_acc.axhline(y=100, color='k', linestyle='--', linewidth=0.5)\n",
    "ax_acc.set_title('Training Accuracy')\n",
    "ax_acc.set_ylabel('Accuracy')\n",
    "ax_acc.set_xlabel('Epoch')\n",
    "ax_acc.legend();\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9277fa91-10da-4a44-9578-ded4dc54e55f",
   "metadata": {},
   "source": [
    "Things look reasonable on a first pass. We are inspecting to see if validation loss/accuracy starts to increase/decrease while training loss continues to decrease/increase. This would be a classic sign of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf6de90-01d1-4590-9bb3-e13c2c5f69db",
   "metadata": {},
   "source": [
    "# 7. Save the network\n",
    "Docs on this: https://pytorch.org/tutorials/beginner/saving_loading_models.html \n",
    "\n",
    "Temporary until we have proper checkpoint save/load built into training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bbe46b-4538-4918-b37a-3040eb255a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_save_name = r\"resnet18_final.pth\" #\n",
    "checkpoint_save_path = project_models_dir / checkpoint_save_name\n",
    "checkpoint_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac193ce-32f4-45ca-8de8-10b7e3af8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model': trained_model,   # Model parameters\n",
    "            'optimizer': optimizer,  # Optimizer parameters\n",
    "            'epochs': num_epochs,  \n",
    "            'train_loss': train_history['train_loss'],\n",
    "            'val_loss': train_history['val_loss'],  \n",
    "            'train_accuracy': train_history['train_topk_accuracy'],\n",
    "            'val_accuracy': train_history['val_topk_accuracy'],\n",
    "            'topk': topk,}, checkpoint_save_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1785e9a-1045-4568-8ec9-3a336712a3db",
   "metadata": {},
   "source": [
    "## Load network (optional)\n",
    "To save time, you can skip from Step 1 to this step once you have trained the network once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aabdb1-480d-4cef-9453-dce0a325ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "if load_model:\n",
    "    # initialize things\n",
    "    print(\"Loading final model checkpoint\")\n",
    "    checkpoint_load_name = r\"resnet18_final.pth\" #\n",
    "    checkpoint_load_path = project_models_dir / checkpoint_load_name\n",
    "    device = 'cuda'\n",
    "    if checkpoint_load_path.exists() and torch.cuda.is_available():\n",
    "        print(\"model exists, cuda available.\")\n",
    "\n",
    "    # load the data\n",
    "    final_checkpoint = torch.load(checkpoint_load_path, weights_only=False)\n",
    "\n",
    "    # Unpack values you want (TODO: cut some of these you don't use)\n",
    "    trained_model = final_checkpoint['model']\n",
    "    optimizer = final_checkpoint['optimizer']\n",
    "    num_epochs = final_checkpoint['epochs']\n",
    "    train_loss = final_checkpoint['train_loss']\n",
    "    val_loss = final_checkpoint['val_loss']\n",
    "    train_topk_accuracy = final_checkpoint['train_accuracy']\n",
    "    val_topk_accuracy = final_checkpoint['val_accuracy']\n",
    "    topk = final_checkpoint['topk']\n",
    "else:\n",
    "    print(\"Not loading model -- likely a training run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07b4cfc-a46a-457f-a84e-04ea8b0c5345",
   "metadata": {},
   "source": [
    "# 8. Check model performance\n",
    "First let's visually inspect model performance over some random images from the validation data set. Then we'll look at some metrics over the entire validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82bdfe5-ea5a-48e6-8638-7adaefdca7cc",
   "metadata": {},
   "source": [
    "## Inspect some predictions\n",
    "We'll use a few deepglue convenience functions to get a few random images and predict their identity using the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2322b0aa-5860-444b-8641-ebb9593beeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_paths, rand_categories = dg.sample_random_images(data_dir, \n",
    "                                                      category_map, \n",
    "                                                      split_type='valid', \n",
    "                                                      num_images=10)\n",
    "random_stack = dg.load_images_for_model(rand_paths, \n",
    "                                        data_transform(train=False)); # nondistorting transform\n",
    "predicted_probs = dg.predict_batch(trained_model, random_stack); # defaults to 'cuda' device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2bbb12-d8bb-46df-a14b-6e3e4af51f2e",
   "metadata": {},
   "source": [
    "Using `dg.plot_prediction_grid()`, we'll plot the actual image and top prediction on the left, and the `top_n` predictions with their probabilities on the right: let's check out the top five predictions of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15649507-2f2b-470d-b997-7d40060a0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg.plot_prediction_grid(random_stack, \n",
    "                        predicted_probs, \n",
    "                        rand_categories, \n",
    "                        category_map, \n",
    "                        top_n=5, \n",
    "                        figsize_per_plot=(2, 2), \n",
    "                        logscale=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9bd491-0462-4e63-8ada-20c5fc10420e",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "While images are good (and we get good performance on individuals), we should examine performance over the entire validation data set (10k images). \n",
    "\n",
    "Scikit learn has many metrics we can use. We have already built a dataloader for validation data above, and deepglue has a function for predicting all the data given a dataloader and the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bb9efd-0395-4fd4-8979-60df95966abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds, val_labels, val_probs = dg.predict_all(trained_model, valid_data_loader);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c992b-af0b-4e6d-a546-4ce8eef3ffa7",
   "metadata": {},
   "source": [
    "Now, with the predictions, correct labels, we can get lots of metrics. We'll focus on some of the basic metrics for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781fa1e7-ca76-4dbf-a837-75875f123a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e1c4f-7e0c-469b-a59d-fe9416c12f23",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "The confusion matrix is very helpful as a visual display: it tells you the count of the actual category and predicted category for all ten categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fb29d3-eabd-426e-b97c-9ddef8633823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm_disp = ConfusionMatrixDisplay.from_predictions(val_labels,\n",
    "                                                  val_preds,\n",
    "                                                  display_labels=category_names,\n",
    "                                                  xticks_rotation=45.,\n",
    "                                                  cmap='magma');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa831dd7-ec91-4a00-8be4-656f29bad58b",
   "metadata": {},
   "source": [
    "### Classification report\n",
    "Scikit learn generates a useful classification report that tells you the precision (indicates levels of false positives along the column of the confusion matrix) and recall (indicates levels of false negatives along the row of the confusion matrix) for each category. F1 is a combination of both precision and recall.  \n",
    "\n",
    "The classification report also aggregates these measures into overall accuracy (overall proportion correct), average precision and recall (called 'macro average'), and weighted average (weighted by class size, which would be useful for imbalanced data). \n",
    "\n",
    "For more on the classification report, there is a useful discussion here: https://www.nb-data.com/p/breaking-down-the-classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd26b3-be34-48e1-9764-2398b375f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5722cd2e-3fd6-481a-8f14-92d7f3f624b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(val_labels, val_preds, target_names=category_names);\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df85c23-29c6-4450-b338-a3c6e93258e7",
   "metadata": {},
   "source": [
    "You can see that some of the categories are classified nearly perfectly (trousers, bags). While others are really tough, in particular shirts. Is this because our network needs more training, resnet18 is not up to the task, or maybe something intrinsic to our dataset? \n",
    "\n",
    "Before investing a ton of time tweaking parameters, let's do some visualization of our data and feature space to find the error patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffffb0ea-dabc-4b6d-b7f8-9ab819a1e968",
   "metadata": {},
   "source": [
    "# 9. Visualize clustering in feature space\n",
    "As just mentioned, while our network is performing very well, we have a decision to make. Should we tweak some parameters? Should we bring in some more fancy learning rate scheduler? Maybe train for more epochs? Some more heavy augmentation in our transformer might be helpful. \n",
    "\n",
    "These are all reasonable options, but this notebook is meant to demo deepglue basics, not deep learning weeds. Also, sometimes it is helpful to visualize what's happening inside of a network before spending that 80% of your time eking out that 2% improvement in the model performance. Pytorch provides some useful tools to extract the activity (features) inside networks, and deepglue has utilities for visualizing the features embedded there. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b90f06-1b03-4c8a-8e27-d4a5a4901bb1",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "The way feature extraction works is run the image(s) through the network, and extract the activity patterns in the desired layer(s) for each image. \n",
    "\n",
    "During normal network operation, we only care about the final prediction of the network, that's what is returned as output from the network. But for analysis of the internals of the network, `pytorch` provides internal hooks to the nodes as a convenience function using `create_feature_extractor()`. We can feed this function images and the network will return the activation values from *any node* of the network when given those images. You feed it batches through a data loader, give it the full data set, but then you've cracked open the black box and can visualize the internal operations of the network.\n",
    "\n",
    "We first need to create a data loader for the feature extractor: deepglue's `prepare_ordered_data()` creates a dataloader for feature extraction that will go in order through all the data in a data split, in order, and return the data loader and the image paths: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf9bd0-3ba3-4e31-b701-b73af2c67cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths, ordered_loader = dg.prepare_ordered_data(data_dir, \n",
    "                                                      image_transforms['valid'], \n",
    "                                                      num_workers=4, \n",
    "                                                      batch_size=56, \n",
    "                                                      split_type='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f64264-3538-4e46-b9aa-a51c84dec04b",
   "metadata": {},
   "source": [
    "We have to decide which layer(s) you want to extract features from. \n",
    "\n",
    "In torchvision, the convention is that the variable `return_nodes` defines the layers from which features can subsequently be extracted (keys are the actual layer names, and values are the names we give them for access later). As discussed above, we typically just want outputs returned from the final output nodes, and this lets us get data from anywhere in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9ac594-5846-47b3-af50-5d5ff8a4a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_nodes = {'relu': 'cnn_features',                  # Early traditional CNN features\n",
    "                'layer1.1.relu': 'resnetL1_features',\n",
    "                'layer2.1.relu': 'resnetL2_features',\n",
    "                'layer3.1.relu': 'resnetL3_features',\n",
    "                'layer4.1.relu': 'resnetL4_features'}\n",
    "pprint(return_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1fde8f-2ad5-4da9-8f8c-da0b0ca87b06",
   "metadata": {},
   "source": [
    "To list all of the options for extracting features you could run the following code (expect a long printout):\n",
    "\n",
    "    for name, module in trained_model.named_modules():\n",
    "        print(name)\n",
    "\n",
    "We are going to keep it simple and just examine the activity very late to visualize clustering of higher level abstract features. Note if you were using `resnet50`, there would be *many* more layers to choose from.\n",
    "\n",
    "Note the earliest one (relu) is from the first convolutional neural network that don't include any residual blocks, so it's considered a traditional feature extraction layer that would provide access to very low-level features (beware there would be *lots* of dimensions so you would needs lots of RAM). The other four are the outputs of the four residual blocks (we expect higher-level fashion features in layer 4 so will extract those in what follows).\n",
    "\n",
    "We use the built-in `torchvision` function `create_feature_extractor()` to create the feature extractor: it just needs our trained model and dictionary of return nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8be5ec-652a-462a-af61-df39be2c2bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.eval();\n",
    "feature_extractor = create_feature_extractor(trained_model.to('cuda'), \n",
    "                                             return_nodes=return_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46071b8f-4aa5-462b-83f6-996424a24c18",
   "metadata": {},
   "source": [
    "Run feature extraction on chosen layer. Note this can take a while because it is running inference on the entire validation dataset so it can extract the activation patterns from the selected layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b20e5-5d07-4810-8b92-389dcf3d242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'resnetL4_features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e448dee6-73e7-49bc-a9de-33534412612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = dg.extract_features(ordered_loader, \n",
    "                                       feature_extractor, \n",
    "                                       layer=layer,\n",
    "                                       device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d73f1-6256-4365-9648-a5802d628da8",
   "metadata": {},
   "source": [
    "The features extracted are fairly high dimension (over 25k dimensions). This is actually much lower than if we had extracted features from earlier layers in the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0ce80b-45bf-4d37-89e8-af067535d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7fa759-38b4-499c-86e0-3a9e5790f96d",
   "metadata": {},
   "source": [
    "## Dimensionality reduction and visualization\n",
    "It is hard to visualize tens of thousands of features. We will now use umap to project these high-dimensional activity patterns to a 2d space to visualize how much they are clustering according to category. We will render this using static plot (Matplotlib) and dynamically (using Bokeh), so when you hover over the scatter points it shows the image, so you can see how the points between clusters look. \n",
    "\n",
    "The following visualizations are adapted from a umap demo: https://umap-learn.readthedocs.io/en/latest/basic_usage.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2482e021-f644-4a59-86fd-c7a9c9c8920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = umap.UMAP(n_components=2, \n",
    "                       n_epochs=200, \n",
    "                       low_memory=True,\n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c7482-68be-44c0-88d8-cb2e7c79ceb7",
   "metadata": {},
   "source": [
    "UMAP is itself an iterative ML algorithm, so the following will take a while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13766c08-f12d-4396-84bf-3e6db24f8122",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "layer_features_umap = umap_model.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b111fa1-6475-4ada-ad39-d56fc4dd0ae7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3 style=\"margin: 1px 0 6px 0;\">Other dimensionality reduction techniques</h3>\n",
    "\n",
    "If you want to compare UMAP to PCA or other dimensionality reduction techniques, it should flow nicely through in the code. For instance:    \n",
    "<code>\n",
    "pca = PCA(n_components=2)\n",
    "layer_features_pca = pca.fit_transform(features)\n",
    "</code>\n",
    "    \n",
    "You can then just replace `layer_features_umap` with `layer_features_pca` in the code. PCA is much faster, and will project based just on variance which is easier to interpret (the clusters won't look as nice though). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f5be3c-e32d-44f7-9720-1f80f11cdf84",
   "metadata": {},
   "source": [
    "### Static visualization\n",
    "Let's first do a static visualization of the feature clusters using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a41a1-a6dc-4f45-a544-26318c923aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Scatter plot of the subsampled features\n",
    "scatter = plt.scatter(\n",
    "    layer_features_umap[:, 0],  # x-coordinates from UMAP\n",
    "    layer_features_umap[:, 1],  # y-coordinates from UMAP\n",
    "    c=labels,           # Color code by labels\n",
    "    cmap=plt.cm.tab10,              \n",
    "    s=10,                          \n",
    "    alpha=0.7                      \n",
    ")\n",
    "\n",
    "# Adding a colorbar for the labels\n",
    "colorbar = plt.colorbar(scatter, boundaries=np.arange(num_classes + 1) - 0.5)\n",
    "colorbar.set_ticks(np.arange(num_classes))\n",
    "colorbar.set_ticklabels(category_names)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "# Title and labels\n",
    "plt.title('UMAP Projection of Fashion')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "\n",
    "# Show plot\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e119b4-a384-4506-8721-45db9717be7b",
   "metadata": {},
   "source": [
    "We can see patterns that match what we saw with our classification report: trousers and bags clearly distinct, shirts overlapping with other categories. However, it would be nice to have more details. For instance, which of those spots were correct/incorrect? What do the shirts actually look like that are close to coats? Let's build an interactive visualization tool that will give us such details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0036f69-002c-4fdd-9d57-a00a07bd4ed3",
   "metadata": {},
   "source": [
    "### Interactive feature visualization\n",
    "We will build an interactive scatter plot that plots a little embeddable image sprite when we hover over a point in the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5d572-400c-4bb0-817e-35ed5a118e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_to_embed = 333 \n",
    "encoded_image_str = dg.create_embeddable_image(image_paths[ind_to_embed], size=(60,60))\n",
    "HTML(f'<img src=\"{encoded_image_str}\" />')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8fc2ae-2c17-4e30-a718-c889c0c96de5",
   "metadata": {},
   "source": [
    "Is the above a shirt or a tshirt? What criteria were used for these categories anyway? These are things we can dig into a bit in what follows. To get predictions and error patterns that map onto the ordered data loader used for feature extraction, we need to get predictions from the ordered data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b5a03-fef9-46ae-9718-532eceee58f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_predictions, ordered_labels, _ = dg.predict_all(trained_model, ordered_loader);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41f7e21-435a-4dce-8d0d-35e7711b6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BTW, the above was a {category_names[ordered_labels[ind_to_embed]]}\")\n",
    "print(\"How did the network do?\")\n",
    "print(f\"Actual category: {ordered_labels[ind_to_embed]}: Predicted category {ordered_predictions[ind_to_embed]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f33298-2abd-41ca-90f7-72b77de270e9",
   "metadata": {},
   "source": [
    "We can use deepglue's `plot_interactive_projection()` to plot everything we wanted in one plot:\n",
    "- Scatter plot of the features projected in umap space, where hovering shows you see the sprite of the corresponding image.\n",
    "- o's show correct predictions, x's show incorrect predictions so you can see what features confused our network.\n",
    "- You can interact with the plot by zooming, selecting regions that you are interested in (see selection tools on right to determine whether you wheel zoom or box select)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdaafdd-7472-4a51-91cb-e04172c6027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg.plot_interactive_projection(layer_features_umap, \n",
    "                               ordered_labels.numpy(), \n",
    "                               image_paths, # to show sprites\n",
    "                               category_map,\n",
    "                               predictions=ordered_predictions.numpy(), # to differentiate correct/incorrect\n",
    "                               title='UMAP Projection', \n",
    "                               image_size=(75, 75), # size of sprite popups\n",
    "                               plot_size=500, # x/y dims of plot\n",
    "                               show_in_notebook=True) # if not True, will pop up plot in new tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d21443-f09a-4ad5-8367-9c3c37fc87b5",
   "metadata": {},
   "source": [
    "With this visualization we can start to see the range of visual features that are important for each category, and how they start to blend into each other and make category membership determination very hard (even for us). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b554c1-6972-491f-999e-fe5e4d964c3c",
   "metadata": {},
   "source": [
    "# 10. Test data\n",
    "\n",
    "So far, we have purposely held off analyzing the test data. We really want to avoid data leakage, so keep the test data sealed off until we are sure we have the final comparisons we want to make. \n",
    "\n",
    "Once you have put your final candidate model (or models) through the ringer, and narrowed things down to a small set of models (e.g., your best resnet, vgg, whatever, with the best augmentation and learning rate schedulers), we put them to the final test with your test data to see which generalizes best outside of all of our training regime (which includes training and validation splits). At that point it's time to get a fair estimate of how well the model generalizes outside all of your training data. It probably won't be as good as for the validation data, which we used to tweak the model. That's why we set aside the test data and leave it in a lock box until the very end. \n",
    "\n",
    "I'm purposely leaving it out of this notebook to give people a chance to try different models (e.g., `resnet50`), add different augmentations, tweak things, improve the model, before doing this. You can then apply the same metrics we went over above with validation data (accuracy, recall, etc) to the test data and see how well things generalized. \n",
    "\n",
    "This is left as an exercise for the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eb1187-27a2-4b1b-98ef-396f6b4de39d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
